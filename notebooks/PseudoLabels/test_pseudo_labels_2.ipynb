{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/jonfrey/ASL\")\n",
    "sys.path.append(\"\"\"/home/jonfrey/ASL/src/\"\"\")\n",
    "sys.path.append(\"\"\"/home/jonfrey/ASL/src/pseudo_label\"\"\")\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "import time\n",
    "from torchvision import transforms as tf\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import one_hot\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import yaml\n",
    "import coloredlogs\n",
    "coloredlogs.install()\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "# LOAD MODULES\n",
    "from pseudo_label.yolo import YoloHelper\n",
    "from pseudo_label.deeplab import DeeplabHelper\n",
    "from pseudo_label.fast_scnn import FastSCNNHelper\n",
    "from visu import Visualizer\n",
    "from datasets_asl import get_dataset\n",
    "from utils_asl import load_yaml\n",
    "from pseudo_label import *\n",
    "from pseudo_label import readSegmentation\n",
    "\n",
    "def get_max_acc(label, gt , names = ['deeplab', 'yolo', 'fastscnn', 'gt'] ):\n",
    "    m = gt != -1\n",
    "    acc_indi = {}\n",
    "    for l,n in zip(label,names):\n",
    "        correct = ( l[m] == gt[m]).sum()\n",
    "        m2 = m * (l != -1)\n",
    "        total = m2.sum()\n",
    "        if total != 0:\n",
    "            acc_indi[n] = correct/total\n",
    "        \n",
    "    # Optimal upper bound\n",
    "    m = gt != -1\n",
    "    m_correct = np.zeros( label[0].shape )\n",
    "    for l,n in zip(label,names):\n",
    "        m_est = l == gt\n",
    "        m_correct[m_est] = 1\n",
    "        \n",
    "    acc_upper_bound = m_correct[m].sum() / m.sum()\n",
    "    acc_indi['upper_bound'] = acc_upper_bound\n",
    "    \n",
    "    return acc_indi\n",
    "\n",
    "\n",
    "class PseudoLabelLoaderOnline():\n",
    "    def __init__(self, base_flow, image_paths, h=960, w=1280, sub=10):\n",
    "        self.image_paths = image_paths\n",
    "        self.sub = sub\n",
    "        self.base_flow = base_flow\n",
    "        self.H, self.W= h,w \n",
    "        \n",
    "    def get_flow(self, global_idx):\n",
    "        flow = []\n",
    "        for idx in global_idx:\n",
    "            fp = os.path.join( \n",
    "                self.base_flow, \n",
    "                dataset_test.image_pths[ idx].split('/')[-3], \n",
    "                f'flow_sub_{self.sub}',\n",
    "                dataset_test.image_pths[ idx].split('/')[-1][:-4]+'.png' )\n",
    "            try:\n",
    "                flow.append( readFlowKITTI( fp, H=self.H ,W=self.W))\n",
    "            except:\n",
    "                return False, False \n",
    "        flow.reverse()\n",
    "        return True, flow\n",
    "    \n",
    "def plot_pseudo_labes( res ):\n",
    "    key_list = list(res.keys())\n",
    "    s = int( len( key_list ) ** 0.5 )\n",
    "    if len( key_list ) - s*s != 0:\n",
    "        s +=1\n",
    "    ba = torch.zeros( (int(s*s),3, *res[key_list[0]].shape), dtype= torch.float32 )\n",
    "    for i in range( len( key_list ) ):\n",
    "        k = key_list[i]\n",
    "        if k != 'img':\n",
    "            img = visu.plot_segmentation( seg=res[k]+1 )\n",
    "        else:\n",
    "            img = res[k]\n",
    "        \n",
    "        img = Image.fromarray(img)\n",
    "        img = img.convert(\"RGBA\")\n",
    "        d = ImageDraw.Draw(img)\n",
    "        fnt = ImageFont.truetype(\"/usr/share/fonts/truetype/ttf-dejavu/DejaVuSansMono-Bold.ttf\", 50)   \n",
    "        d.rectangle(((550, 500), (1400, 600)), fill=(254,10,10))\n",
    "        d.text((600,530), k , font=fnt, fill=(254,254,254,254))\n",
    "        img =  img.convert(\"RGB\")\n",
    "        img = np.array( img )\n",
    "        ba[i,:] = torch.from_numpy( img[:,:,:3] ).permute(2,0,1)\n",
    "    \n",
    "    grid_ba = make_grid( ba ,nrow = s ,padding = 2,\n",
    "      scale_each = False, pad_value = -1)\n",
    "    visu.plot_image(img = grid_ba +1 , jupyter=True)\n",
    "\n",
    "def print_acc(acc_dict, counts_flow, counts):\n",
    "    avg = {}\n",
    "    for k in acc_dict.keys():\n",
    "        if k.find('flow') != -1:\n",
    "            avg[k]  = acc_dict[k] / counts_flow\n",
    "        else:\n",
    "            avg[k] = acc_dict[k] / counts\n",
    "    print(avg)\n",
    "    \n",
    "def valid_sequential_indi(global_idx_list, globale_idx_to_image_path ):\n",
    "  v = global_idx_list[0]\n",
    "  \n",
    "  prev = globale_idx_to_image_path[0].split('/')[-1][:-4]\n",
    "  print(prev)\n",
    "  prev = int( prev.replace(\"undistorted_frame\", \"\") )\n",
    "  print(prev)  \n",
    "  for g in global_idx_list[1:]:\n",
    "      g = globale_idx_to_image_path[g].split('/')[-1][:-4]\n",
    "      print(g)\n",
    "      g = int( g.replace(\"undistorted_frame\", \"\") )\n",
    "      print(g)\n",
    "      if g != prev + sub:\n",
    "          suc = False\n",
    "          break\n",
    "      prev = g\n",
    "  return True\n",
    "\n",
    "def plot_pseudo_labes( res, visu, counts_flow, jupyter  ):\n",
    "    key_list = list(res.keys())\n",
    "    s1 = int( len( key_list ) ** 0.5 )\n",
    "    s2 = 1\n",
    "    \n",
    "    while s1 * s2 < len( key_list):\n",
    "        s2 +=1\n",
    "    \n",
    "    ba = torch.zeros( (int(s1*s2),3, *res[key_list[0]].shape), dtype= torch.float32 )\n",
    "    for i in range( len( key_list ) ):\n",
    "        k = key_list[i]\n",
    "        if k.find('img') == -1:\n",
    "            img = visu.plot_segmentation( seg=res[k]+1 )\n",
    "        else:\n",
    "            img = res[k]\n",
    "        \n",
    "        img = Image.fromarray(img)\n",
    "        img = img.convert(\"RGBA\")\n",
    "        d = ImageDraw.Draw(img)\n",
    "        fnt = ImageFont.truetype(\"/usr/share/fonts/truetype/ttf-dejavu/DejaVuSansMono-Bold.ttf\", 50)   \n",
    "        d.rectangle(((550, 500), (1400, 600)), fill=(254,10,10))\n",
    "        d.text((600,530), k , font=fnt, fill=(254,254,254,254))\n",
    "        img =  img.convert(\"RGB\")\n",
    "        img = np.array( img )\n",
    "        ba[i,:] = torch.from_numpy( img[:,:,:3] ).permute(2,0,1)\n",
    "    \n",
    "    grid_ba = make_grid( ba ,nrow = s1 ,padding = 2,\n",
    "      scale_each = False, pad_value = -1)\n",
    "    visu.plot_image(img = grid_ba +1 , store=True, jupyter=jupyter, tag=\"Label_Generation_sub1_{:05d}\".format(counts_flow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabelGenerator():\n",
    "    def __init__(self, base_path, sub=10, confidence='equal', \n",
    "            flow_mode='sequential', H=640, W=1280, \n",
    "            nc=40, window_size=10,\n",
    "            visu=None, pre_fusion_function=None, visu_active=True, cfg_loader={}):\n",
    "        \"\"\"  \n",
    "        confidence:\n",
    "          'equal': perfect optical flow -> all project labels are equally good\n",
    "          'linear': linear rate -> per frame\n",
    "          'exponential': exponential rate -> per frame \n",
    "        flow_mode:\n",
    "          'sequential': #-> 0->1, 1->2, 2->3\n",
    "          'target': 0->3 1->3 2->3\n",
    "        \"\"\"\n",
    "        self._visu_active = visu_active\n",
    "        self._sub = sub\n",
    "        self._flow_mode = flow_mode #'sequential' #-> 0->1, 1->2, 2->3 # 'target' 0->3 1->3 2->3\n",
    "        self._H,self._W = H,W\n",
    "        self._confidence= confidence # equal, linear, exponential\n",
    "        self._nc = nc\n",
    "        self._window_size = window_size\n",
    "        # Passed externally\n",
    "        self._visu = visu\n",
    "    \n",
    "    def calculate_label(self, index=None, seg=[],flow=[], image= None ):\n",
    "        if not index is None:\n",
    "            seg_forwarded= self._forward_index(index) #return H,W,C\n",
    "        else:\n",
    "            seg_forwarded = self._forward_index(index, seg, flow)\n",
    "        # -1 39 -> 0 -> 40 We assume that the network is also able to predict the invalid class\n",
    "        # In reality this is not the case but this way we can use the ground truth labels for testing\n",
    "        confidence_values_list = self._get_confidence_values(seq_length= len(seg_forwarded))\n",
    "        res = np.array(confidence_values_list)[:,None,None,None] * np.stack( seg_forwarded )\n",
    "        res = res.sum(axis=0)\n",
    "        res = np.moveaxis( res, [0,1,2], [1,2,0])\n",
    "        return res\n",
    "\n",
    "    def _get_confidence_values( self, seq_length ):\n",
    "        if self._confidence == 'equal':\n",
    "            return [float( 1/seq_length)] * seq_length \n",
    "\n",
    "        if self._confidence == 'linear':\n",
    "            ret = []\n",
    "            lin_rate = 0.1\n",
    "            s = 0\n",
    "            for i in range(seq_length):\n",
    "                res = 1 - lin_rate* (seq_length-i)\n",
    "                if res < 0: \n",
    "                    res = 0\n",
    "                s += res\n",
    "\n",
    "                ret.append(res)\n",
    "            return [r/s for r in ret]\n",
    "\n",
    "        elif self._confidence == 'exponential':\n",
    "            ret = []\n",
    "            exp_rate = 0.8\n",
    "            s = 0\n",
    "            for i in range(seq_length):\n",
    "                res = exp_rate**(seq_length-i)\n",
    "                if res < 0: \n",
    "                    res = 0\n",
    "                s += res\n",
    "                ret.append(res)\n",
    "            return [r/s for r in ret]\n",
    "\n",
    "\n",
    "    def _forward_index(self, index=None, seg=[],flow=[] ,pre_fusion_function=None ):\n",
    "        \"\"\"\n",
    "        seg[0] , C,H,W\n",
    "        \n",
    "        pre_fusion_function should be used to integrate the depth measurments \n",
    "        to the semseg before forward projection !\n",
    "\n",
    "        seg_forwarded[0] -> oldest_frame\n",
    "        seg_forwarded[len(seg_forwarded)] -> latest_frame not forwarded\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if len( seg[0].shape ) == 3 and seg[0].shape[0] != 1:\n",
    "            soft = True\n",
    "        else:\n",
    "            soft = False\n",
    "        \n",
    "        seg_forwarded = []\n",
    "        for j in range(len( seg )):\n",
    "            seg[j] = np.moveaxis( seg[j], [0,1,2], [2,0,1] ) #C,H,W -> H,W,C\n",
    "        \n",
    "        for i in range(0,len(seg)-1):\n",
    "            i = len(seg)-1-i\n",
    "            seg_forwarded.append( seg[i].astype(np.float32) )\n",
    "            \n",
    "            # CREATE FLOW MAP\n",
    "            if i != 0:\n",
    "                f = flow[i][0]\n",
    "            else:\n",
    "                f = np.zeros(flow[i][0].shape, dtype=np.float32)\n",
    "            h_, w_ = np.mgrid[0:self._H, 0:self._W].astype(np.float32)\n",
    "            h_ -= f[:,:,1]\n",
    "            w_ -= f[:,:,0]\n",
    "            \n",
    "            # FORWARD ALL PAST FRAMES\n",
    "            j = 0\n",
    "            for s in seg_forwarded :\n",
    "                if soft:\n",
    "                    s = cv2.remap( s, w_, h_, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "                else:\n",
    "                    s = cv2.remap( s[None], w_, h_, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=-1)[None]\n",
    "                seg_forwarded[j] = s\n",
    "                j += 1\n",
    "        \n",
    "        seg_forwarded.append( seg[0].astype(np.float32) )\n",
    "        return seg_forwarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "visu = Visualizer(os.getenv('HOME')+'/tmp', logger=None, epoch=0, store=False, num_classes=41)\n",
    "\n",
    "yh = YoloHelper()\n",
    "dlh = DeeplabHelper(device=DEVICE)\n",
    "fsh = FastSCNNHelper(device=DEVICE)\n",
    "\n",
    "\n",
    "env_cfg_path = os.path.join('cfg/env', os.environ['ENV_WORKSTATION_NAME']+ '.yml')\n",
    "env_cfg = load_yaml(env_cfg_path)\n",
    "\n",
    "# /home/jonfrey/ASL/cfg/eval/eval_labdata.yml\n",
    "# /home/jonfrey/ASL/cfg/eval/eval.yml SCANNET\n",
    "eval_cfg = load_yaml(\"/home/jonfrey/ASL/cfg/eval/eval_labdata.yml\")\n",
    "\n",
    "# SETUP DATALOADER\n",
    "dataset_test = get_dataset(\n",
    "  **eval_cfg['dataset'],\n",
    "  env = env_cfg,\n",
    "  output_trafo = None,\n",
    "  )\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "  shuffle = False,\n",
    "  num_workers = 0,\n",
    "  pin_memory = eval_cfg['loader']['pin_memory'],\n",
    "  batch_size = 1, \n",
    "  drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "globale_idx_to_image_path = dataset_test.image_pths\n",
    "\n",
    "# /media/scratch1/jonfrey/models/master_thesis/flow_labdata/labdata\n",
    "# /home/jonfrey/ASL/cfg/eval/eval_labdata.yml\n",
    "PseudoLabelLoaderOnline(base_flow = '/home/jonfrey/results/scannet_pseudo_label/scannet',\n",
    "                      image_paths = dataset_test.image_pths,\n",
    "                      sub=1)\n",
    "\n",
    "st = time.time()\n",
    "counts = 0\n",
    "counts_flow = 0\n",
    "length = len(dataloader_test)\n",
    "segmentation_list_fastscnn = []\n",
    "global_idx_list = []\n",
    "\n",
    "plot = False\n",
    "# PARAMS LABEL GENERATION\n",
    "weights = [2,0.5,1]\n",
    "weights_temperature = [1,1,1]\n",
    "weights_acc_based = [0.68,0.2,0.5]\n",
    "window_size_2 = 2\n",
    "window_size = 3\n",
    "sub = 1\n",
    "acc_dict = {}\n",
    "# '/home/jonfrey/results/scannet_pseudo_label/scannet'\n",
    "pa = \"/media/scratch1/jonfrey/models/master_thesis/flow_labdata/labdata\"\n",
    "plg = PseudoLabelGenerator(base_path=pa, \n",
    "                        visu=visu,\n",
    "                        window_size=window_size,\n",
    "                        visu_active=plot)\n",
    "\n",
    "plg_linear_decay = PseudoLabelGenerator(base_path=pa, \n",
    "                        visu=visu,\n",
    "                        window_size=window_size,\n",
    "                        visu_active=plot,\n",
    "                        confidence = 'linear')\n",
    "\n",
    "\n",
    "plg_ws_2 = PseudoLabelGenerator(base_path=pa, \n",
    "                        visu=visu,\n",
    "                        window_size=window_size_2,\n",
    "                        visu_active=plot)\n",
    "\n",
    "plg_super = PseudoLabelGenerator(base_path=pa, \n",
    "                        visu=visu,\n",
    "                        window_size=window_size,\n",
    "                        visu_active=plot)\n",
    "\n",
    "pllo = PseudoLabelLoaderOnline(base_flow =pa,\n",
    "                      image_paths = dataset_test.image_pths,\n",
    "                      sub=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.image_pths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_softmax_temperature(softmax_output, beta):\n",
    "    # C, H, W\n",
    "    x = np.log( softmax_output ) # get the network output up to a constant C\n",
    "    expo = np.exp(x*beta)\n",
    "    return expo / np.sum(expo, axis= 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = tf.Resize((320,640))\n",
    "up = tf.Resize((640,1280))\n",
    "label_list = []\n",
    "\n",
    "fast_run = False\n",
    "stop = 5000\n",
    "jupyter = True\n",
    "suc =False\n",
    "for j, batch in enumerate( dataloader_test ):\n",
    "  # START EVALUATION  \n",
    "  images = batch[0]\n",
    "  target = batch[1]\n",
    "  ori_img = batch[2]\n",
    "  replayed = batch[3]\n",
    "  BS = images.shape[0]\n",
    "  global_idx = batch[4] \n",
    "  images *= 255\n",
    "  images = images.permute(0,2,3,1).numpy().astype(np.uint8)\n",
    "  pri = False\n",
    "\n",
    "  for b in range( images.shape[0] ):\n",
    "      # EVALUATE SEMANTIC SEGMENTATION NETWORKS\n",
    "      print(j)\n",
    "      probs = {}\n",
    "      st_ = time.time()\n",
    "      prob_dl = dlh.get_label_prob( images[b] )\n",
    "      prob_yolo = yh.get_label_prob( images[b] )\n",
    "        \n",
    "      inp = down(torch.from_numpy( images[b]).permute(2,0,1)).permute(1,2,0).numpy()\n",
    "      prob_fastscnn = fsh.get_label_prob( inp )\n",
    "      prob_fastscnn = up ( torch.from_numpy( prob_fastscnn[None]) )[0].numpy()\n",
    "\n",
    "      probs['dl'] = prob_dl\n",
    "      probs['yolo'] = prob_yolo\n",
    "      probs['fastscnn'] = prob_fastscnn\n",
    "      \n",
    "      prob_sum = prob_dl + prob_yolo + prob_fastscnn\n",
    "      probs['sum'] = prob_sum\n",
    "      \n",
    "      prob_weighted_sum = weights[0] * prob_dl + weights[1] * prob_yolo + weights[2] * prob_fastscnn\n",
    "      probs['weighted_sum'] = prob_weighted_sum\n",
    "      \n",
    "      prob_weighted_acc = weights_acc_based[0] * prob_dl + weights_acc_based[1] * prob_yolo + weights_acc_based[2] * prob_fastscnn\n",
    "      probs['weighted_acc'] = prob_weighted_acc\n",
    "        \n",
    "      prob_weighted_sum_temperature = weights_temperature [0] * prob_dl + weights_temperature[1] * prob_yolo + weights_temperature[2] * prob_fastscnn\n",
    "      probs['weighted_sum_temperature'] = prob_weighted_sum_temperature\n",
    "        \n",
    "      if not fast_run:\n",
    "          # RINGBUFFER THE PREDICTIONS\n",
    "          label_list.append( copy.deepcopy( probs) )\n",
    "          global_idx_list.append(int( global_idx[b] ))\n",
    "          if len(label_list) > window_size:\n",
    "              label_list = label_list[-window_size:]\n",
    "              global_idx_list = global_idx_list[-window_size:]\n",
    "              # GET THE FLOW BETWEEN THE FRAMES\n",
    "              suc, flow_list = pllo.get_flow( global_idx = global_idx_list)\n",
    "              print(\"Suc 1\", suc)\n",
    "              # CHECK IF THE GLOBAL IDX LIST ALIGNS\n",
    "              suc_2 = valid_sequential_indi( global_idx_list, dataset_test.image_pths )\n",
    "              suc = suc and suc_2\n",
    "              print(\"Suc 2\", suc)\n",
    "              # CREATE PSEUDO LABEL\n",
    "              if suc:\n",
    "                  for k in list( [\"weighted_sum\",'dl',\"fastscnn\"] ): \n",
    "                        #label_list[0][k] should be C,H,W or H,W\n",
    "                        if len(label_list[0][k].shape) != 3:\n",
    "                            # make onehot encoding\n",
    "                            seg = [ s[k][None] for s in label_list ]\n",
    "                        else:\n",
    "                            seg = [ s[k] for s in label_list ]\n",
    "                        seg.reverse()\n",
    "\n",
    "                        probs[k+f'_flow_ws_{window_size}']= plg.calculate_label(\n",
    "                          index=None, \n",
    "                          seg= copy.deepcopy( seg ) , \n",
    "                          flow= flow_list)\n",
    "\n",
    "                        probs[k+'_flow_linear'] = plg_linear_decay.calculate_label(\n",
    "                          index=None, \n",
    "                          seg= copy.deepcopy( seg ) , \n",
    "                          flow= flow_list)\n",
    "\n",
    "                        probs[k+f'_flow_ws_{window_size_2}'] = plg_ws_2.calculate_label(\n",
    "                          index=None, \n",
    "                          seg= copy.deepcopy( seg ) , \n",
    "                          flow= flow_list)\n",
    "\n",
    "                  pri =True\n",
    "                  counts_flow += 1\n",
    "\n",
    "      # transform porbs to labels\n",
    "      for k in probs.keys():\n",
    "        if probs[k].shape[0] == 41:\n",
    "            invalid_labels = probs[k][1:].sum(axis=0) == 0            \n",
    "            probs[k] = np.argmax( probs[k] [1:,:,:], axis=0 )\n",
    "            probs[k][ invalid_labels ] = -1 \n",
    "      \n",
    "      # EVALUATE ALL LABELS\n",
    "      ret = get_max_acc(\n",
    "              label = list( probs.values()) , \n",
    "              gt=target[b].numpy(), \n",
    "              names= list( probs.keys()))\n",
    "      print(suc)\n",
    "      if suc: # or fast_run:  \n",
    "          try: \n",
    "              probs['img_flow'] = np.uint8(visu.plot_flow(flow=flow_list[0][0] ))\n",
    "          except:\n",
    "              pass\n",
    "            \n",
    "          probs['img'] = images[b]\n",
    "          probs['gt'] = target[b].numpy()\n",
    "          print(\"lot\", jupyter)\n",
    "          plot_pseudo_labes( probs, visu, counts_flow, jupyter  )\n",
    "          suc = False\n",
    "    \n",
    "        \n",
    "      for k in ret.keys():\n",
    "          if k in acc_dict:\n",
    "              acc_dict[k] += ret[k]\n",
    "          else:\n",
    "              acc_dict[k] = ret[k]\n",
    "      counts += 1\n",
    "    \n",
    "  gc.collect()\n",
    "  if j % 10 == 0:\n",
    "        m = min(stop, len(dataloader_test))\n",
    "        print( f\"{j}/{m} Progress\")\n",
    "  if j > stop: \n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_acc(acc_dict, counts_flow, counts)\n",
    "weights_acc_based = [ acc_dict['dl'], acc_dict['yolo'], acc_dict['fastscnn'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "((adapt_softmax_temperature( probs['fastscnn'][1:], beta = 1) - probs['fastscnn'][1:] ) > 0.001).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"test\".replace(\"st\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track4",
   "language": "python",
   "name": "track4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
